{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Импортируем модули и данные (с предварительной обработкой)**"
      ],
      "metadata": {
        "id": "DyswfL0KWb8n"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHah9Vq74t0e"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import re\n",
        "import random\n",
        "import tqdm\n",
        "import time"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-bs1_g342Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57897f7-d6f4-44d1-ebe1-378cdf8d0a8b"
      },
      "source": [
        "!wget https://s3.amazonaws.com/text-datasets/nietzsche.txt"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-07 10:45:33--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.10.174, 52.217.113.56, 52.217.84.230, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.10.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: ‘nietzsche.txt.2’\n",
            "\n",
            "nietzsche.txt.2     100%[===================>] 586.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-01-07 10:45:34 (3.84 MB/s) - ‘nietzsche.txt.2’ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArE9Sysh5EDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13023cb8-c904-41ee-ec64-20ccead96d68"
      },
      "source": [
        "with open('nietzsche.txt', encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('length:', len(text))\n",
        "text = re.sub('[^a-z ]', ' ', text)\n",
        "text = re.sub('\\s+', ' ', text)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length: 600893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ijyo7gcL49kz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "26f3bd9d-02d1-4d88-c8ed-7216fdf95472"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'preface supposing that truth is a woman what then is there not ground for suspecting that all philos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Создаем словарь**"
      ],
      "metadata": {
        "id": "ctLILtTmWvs5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNiH2xET5HxF"
      },
      "source": [
        "INDEX_TO_CHAR = sorted(list(set(text)))\n",
        "CHAR_TO_INDEX = {c: i for i, c in enumerate(INDEX_TO_CHAR)}"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(INDEX_TO_CHAR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHw-gEUdbci_",
        "outputId": "8a85caf9-065d-43aa-bb0a-0008634e752f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAer4W2RYfhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62891f2b-6b7d-4606-c3be-65ceb9f7aeff"
      },
      "source": [
        "print(CHAR_TO_INDEX)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Создаем входы и выходы, переводим в цифру**"
      ],
      "metadata": {
        "id": "o7X5uMa0W-fi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4EeJBub5ueL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c10c5b3-e247-441c-c8c2-ea26579c1c94"
      },
      "source": [
        "MAX_LEN = 40\n",
        "STEP = 3\n",
        "SENTENCES = []\n",
        "NEXT_CHARS = []\n",
        "for i in range(0, len(text) - MAX_LEN, STEP):\n",
        "    SENTENCES.append(text[i: i + MAX_LEN])\n",
        "    # созжаем список кусков текста по 40 символов со смещением 3\n",
        "    NEXT_CHARS.append(text[i + MAX_LEN])\n",
        "    # создаем список букв, которые следуют за каждым кускос текста, чтобы их предсказывать\n",
        "print('Num sents:', len(SENTENCES))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num sents: 193075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SENTENCES[:5] # подается на вход"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3TY8C6YTerJ",
        "outputId": "7b9382b3-0c28-45a1-8d2b-110aed1db584"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['preface supposing that truth is a woman ',\n",
              " 'face supposing that truth is a woman wha',\n",
              " 'e supposing that truth is a woman what t',\n",
              " 'upposing that truth is a woman what then',\n",
              " 'osing that truth is a woman what then is']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NEXT_CHARS[:5] # подается на выход"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUrrqawgTlUI",
        "outputId": "d4a54b0d-96ba-4fd6-c3d6-89e7c4efd291"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['w', 't', 'h', ' ', ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHPHQII_6MUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9739e5-8e63-49f4-df97-95978158ef72"
      },
      "source": [
        "# переводим наши вход и выход в цифру и тензор\n",
        "print('Vectorization...')\n",
        "X = torch.zeros((len(SENTENCES), MAX_LEN), dtype=int)\n",
        "Y = torch.zeros((len(SENTENCES)), dtype=int)\n",
        "for i, sentence in enumerate(SENTENCES):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t] = CHAR_TO_INDEX[char]\n",
        "    Y[i] = CHAR_TO_INDEX[NEXT_CHARS[i]]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ4KkQi_ejvw",
        "outputId": "dc629a35-256b-4021-cc95-b51fa094f22f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([193075, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEPjqP82epRN",
        "outputId": "4836ae43-8021-4d76-fe36-7961529c44ae"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([193075])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7MP7Jzi7PAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81b83ab-69da-404c-d547-1278e2429223"
      },
      "source": [
        "# смотрим чо вышло\n",
        "X[0:1], Y[0]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[16, 18,  5,  6,  1,  3,  5,  0, 19, 21, 16, 16, 15, 19,  9, 14,  7,  0,\n",
              "          20,  8,  1, 20,  0, 20, 18, 21, 20,  8,  0,  9, 19,  0,  1,  0, 23, 15,\n",
              "          13,  1, 14,  0]]),\n",
              " tensor(23))"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Создаем загрузчик длиной 512 (батч) для подачи в модель**"
      ],
      "metadata": {
        "id": "rHKZVQk0XjBY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XKb2CyB6nwL"
      },
      "source": [
        "BATCH_SIZE=512\n",
        "dataset = torch.utils.data.TensorDataset(X, Y)\n",
        "data = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data:\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2kwJGyrX6W0",
        "outputId": "7b36f1ea-0166-4709-8327-a9aa45116eb1"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[20, 15,  0,  ...,  8,  5,  0],\n",
            "        [20,  5,  5,  ...,  0,  9, 13],\n",
            "        [21, 19,  9,  ..., 13,  5, 14],\n",
            "        ...,\n",
            "        [12, 19,  5,  ...,  0,  4,  9],\n",
            "        [20,  0,  1,  ...,  5,  0,  8],\n",
            "        [14,  7, 19,  ...,  6,  5,  3]]), tensor([ 9, 16,  4,  2, 16,  4, 20,  4, 13, 15,  0,  5, 10,  5,  0,  0,  3,  0,\n",
            "         3,  9, 19, 23, 15, 14,  3, 14,  0, 19, 21,  0, 22,  0, 19,  9,  5,  8,\n",
            "        12,  8,  9,  1,  0,  2,  2,  9,  0,  0, 23, 19, 15, 20, 18, 25, 15,  5,\n",
            "        21, 20, 15,  9,  1, 12,  0,  0,  5, 20,  0,  5, 14,  8, 19, 15, 15,  7,\n",
            "        16, 18, 12,  0, 13,  9, 21,  9,  0,  0, 12,  5, 20, 18,  0,  0, 19,  0,\n",
            "         0,  0,  0,  2, 16,  7,  9, 14,  0, 20, 19,  5,  0, 15, 14,  5, 18,  4,\n",
            "        12, 21,  3,  0,  5,  7, 20,  5,  6,  0, 12, 20,  0,  5,  1,  0,  1,  1,\n",
            "         0,  5, 14, 16,  5,  5, 14,  7, 18, 12,  1, 15, 13, 13,  3, 15,  5,  5,\n",
            "         5,  4,  0,  6,  4,  6, 18, 19, 21, 15,  9,  0,  1, 23,  1, 12,  5,  0,\n",
            "        24,  1, 15, 20,  3,  0, 18,  0, 19, 15,  9,  6, 21, 15, 12, 15, 14,  1,\n",
            "        19,  9, 14,  5, 19,  4, 12, 20, 15, 15,  0,  9,  0, 19,  5, 14, 15, 18,\n",
            "         8, 18,  0, 15,  5,  8,  0,  0, 20, 25, 20, 19, 16,  8,  1,  4, 21,  1,\n",
            "         5, 14,  5,  0,  0, 16, 20,  0, 14, 20, 12,  0,  0, 15,  0,  8, 14,  0,\n",
            "        15,  1,  5,  0, 15, 23,  8,  5,  5, 20,  4,  9, 20, 18,  0,  0,  0,  0,\n",
            "        20,  3,  4,  7, 15,  0, 15, 20,  0,  9,  1, 14,  0,  5, 16,  5,  0,  0,\n",
            "        18, 12,  0,  0,  2, 13,  1,  5, 19,  4,  0,  5,  9,  2,  3, 12,  0, 18,\n",
            "        15, 21, 14,  0, 20,  8,  8,  9, 14,  5, 14, 14, 18, 16,  5,  9,  3, 19,\n",
            "        14, 20,  1, 15, 23,  0,  5, 12,  1, 18, 21,  0, 16,  5,  0,  0,  8, 20,\n",
            "         5, 23,  0, 20, 14,  5, 25,  9, 23, 14, 18,  5, 23, 18,  5,  9,  5, 22,\n",
            "        14,  9, 15,  0,  0, 19,  0,  8, 20, 16, 20, 25, 21,  1,  0,  5, 19,  1,\n",
            "        20, 15,  5,  5,  5, 15,  0,  8,  0,  0,  4, 19,  0,  5, 19, 18,  5,  0,\n",
            "         0, 18, 12, 16, 19,  4, 15, 20,  0, 20,  1, 23,  3, 20,  0,  5,  5,  5,\n",
            "         0, 14, 19,  0, 13,  0,  5,  9,  0,  8,  1, 20,  3, 20,  0,  0, 19,  8,\n",
            "         0,  8, 15, 20,  9,  6,  0,  1,  0,  0,  0,  4,  5, 18, 12,  5, 20,  0,\n",
            "         0,  1,  0, 13,  9,  8,  0,  0, 15, 25, 20, 15,  2,  8,  8,  0,  0, 15,\n",
            "         0,  0,  0, 14,  5, 20,  1, 21, 19,  0, 15, 16,  0,  9, 19,  5,  5, 15,\n",
            "         9,  1, 20, 20,  5, 12, 19, 21, 24,  5, 18,  9,  1,  0,  0, 14, 15, 20,\n",
            "        15,  0,  2, 19,  0,  1, 15,  1,  5, 15,  1, 19,  1, 20,  0,  0, 19,  5,\n",
            "         5,  5, 18,  5, 13, 19,  1, 20])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psIcSGM27YPL"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
        "                    # тип_сетки, длина словаля, эмбеддинг, количество_слоев(циклов?типа), количество классов)\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_hiddens = num_hiddens # количество_слоев(циклов?типа)\n",
        "        self.embedding = nn.Embedding(dictionary_size, embedding_size) # эмбеддинг для каждой буквы словаря, матрица (27,64)\n",
        "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True) #  модель GRU\n",
        "        self.output = nn.Linear(num_hiddens, num_classes) # линейный слой\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.embedding(X) # эмбединг для входа на основе эмбединга словаря штоли\n",
        "        _, state = self.hidden(out) # скрытое состояние(_) + выход(state)\n",
        "        predictions = self.output(_[:, -1, :].squeeze())#state[0].squeeze()) # предсказание\n",
        "        return predictions"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHDuSE8A7ssc"
      },
      "source": [
        "model = NeuralNetwork(nn.LSTM, len(CHAR_TO_INDEX), 64, 128, len(CHAR_TO_INDEX))"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvKPD9L9zJal",
        "outputId": "aac32626-0448-4534-d71c-3fc9b6b39c1e"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([193075, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Модель выдает список уверенности модели в каждом символе, что он следующий**"
      ],
      "metadata": {
        "id": "A3hlbUgChMmx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTCG-ESC74UK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb2bb52-0c96-4d2a-8d27-382654c4daa8"
      },
      "source": [
        "model(X[0:1])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.0061e-01,  6.6450e-02, -3.7602e-02, -8.7220e-02,  6.0793e-03,\n",
              "        -3.3691e-01,  6.3828e-02,  8.4020e-02, -6.6685e-02, -4.3806e-02,\n",
              "         1.7248e-01,  2.8862e-01, -2.8817e-04,  1.5322e-01,  4.6966e-02,\n",
              "         4.3404e-02, -1.9415e-01, -6.4912e-02,  9.0666e-02,  1.5610e-01,\n",
              "        -7.0881e-02,  7.7629e-02, -6.3254e-02, -6.6895e-02,  1.4259e-02,\n",
              "        -8.0954e-02,  7.0748e-03], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Просто разбираем что и как (отступление)**"
      ],
      "metadata": {
        "id": "lwy1mWLQhddq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "e8Qm8_EqjvLk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gch6FQl8x6Hj"
      },
      "source": [
        "embedding = nn.Embedding(len(INDEX_TO_CHAR), 15)\n",
        "rnn = nn.LSTM(15,128, batch_first=True)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o, s = rnn(embedding(X[0:10]))\n",
        "o.shape, o[:, -1, :].squeeze(), len(s), s[0].shape, s[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJsUqfxYeq0b",
        "outputId": "059d125c-faf7-4053-f147-a4860779843e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 40, 128]),\n",
              " tensor([[-0.0187,  0.0633,  0.1252,  ..., -0.0567,  0.0424,  0.0948],\n",
              "         [-0.1143,  0.0419,  0.1379,  ..., -0.0669,  0.0771,  0.0540],\n",
              "         [-0.0844,  0.0332,  0.1269,  ..., -0.0330,  0.0328,  0.0432],\n",
              "         ...,\n",
              "         [-0.0046,  0.0055,  0.0559,  ..., -0.1016,  0.0666,  0.0771],\n",
              "         [ 0.0681,  0.0301,  0.0838,  ..., -0.0515, -0.0014,  0.1414],\n",
              "         [ 0.0604,  0.0300, -0.0110,  ..., -0.0655,  0.0317,  0.0895]],\n",
              "        grad_fn=<SqueezeBackward0>),\n",
              " 2,\n",
              " torch.Size([1, 10, 128]),\n",
              " torch.Size([1, 10, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU**"
      ],
      "metadata": {
        "id": "fvkStPetj07h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.GRU(15,128, batch_first=True)\n",
        "o, s = rnn(embedding(X[0:10]))\n",
        "o.shape, len(s), s[0].shape"
      ],
      "metadata": {
        "id": "qbiqECBCP4Bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435c6b20-353c-4df2-b49c-3d20cb3686a5"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 40, 128]), 1, torch.Size([10, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evDHlyNOykBr"
      },
      "source": [
        "o, s = rnn(embedding(X[0:10]))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbeKFkwdFclg"
      },
      "source": [
        "model = model.cuda()"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Добавляем вариативность.**\n",
        "def sample(preds) - функиция которая, имея одинаково высокую вероятность выпадения несколькох букв, дает выпадание то одной, то другой буквы, чтобы не было постоянно одного и того же текста"
      ],
      "metadata": {
        "id": "_EFpdHDxTpOa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQpkKJV_76dq"
      },
      "source": [
        "def sample(preds):\n",
        "    softmaxed = torch.softmax(preds, 0)\n",
        "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
        "    return probas.argmax()\n",
        "\n",
        "def generate_text():\n",
        "    start_index = random.randint(0, len(text) - MAX_LEN - 1) # случайным образом задаем стартовый индекс из текста\n",
        "\n",
        "    generated = '' # пустая строка для текста\n",
        "    sentence = text[start_index: start_index + MAX_LEN] # кусок текста от рэндомного индекса длиной 40\n",
        "    generated += sentence # добавляем полученный текст в пустую строку\n",
        "\n",
        "    for i in range(MAX_LEN): # от 0 до 39 включительно\n",
        "        x_pred = torch.zeros((1, MAX_LEN), dtype=int) # нулевой тензор (1,1,40)\n",
        "        for t, char in enumerate(generated[-MAX_LEN:]):\n",
        "            x_pred[0, t] = CHAR_TO_INDEX[char] # пробегаем по нулевому тензору заменяя\n",
        "                                              # нули на индексы букв согласно словарю\n",
        "        preds = model(x_pred.cuda()).cpu() # подаем проиндексированный текст в модель\n",
        "        # preds = model(x_pred.cuda())[0].cpu() это не срабатывало, т.к. на выходе 1 главный элемент, а для sample(preds) нужны все, он сам выберет\n",
        "        next_char = INDEX_TO_CHAR[sample(preds)] # предсказанный индекс переводим в букву согласно словарю\n",
        "        generated = generated + next_char # добавляем букву в сроку (пустую, но она не пуста)\n",
        "    print(generated[:MAX_LEN] + '|' + generated[MAX_LEN:])"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EV09Ast97aQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1cb1d71-7e65-4cc3-eb32-31ed1bf589b5"
      },
      "source": [
        "generate_text()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "or souls into the ultimate intentions of| wfvahegdgfkrdovktc nidvyngwhgtqloyybpbe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hylQYY8H_Lw2"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qshorynU9-Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71ac48f-abf7-4faa-b7d8-8b94a987b91c"
      },
      "source": [
        "for ep in range(100):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "\n",
        "    model.train()\n",
        "    for X_b, y_b in data:\n",
        "        X_b, y_b = X_b.cuda(), y_b.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        answers = model(X_b)\n",
        "        loss = criterion(answers, y_b)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1\n",
        "\n",
        "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
        "    model.eval()\n",
        "    generate_text()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Time: 3.694, Train loss: 2.182\n",
            "europe it is worst and most varied where|n of way at the hecrerebyraintuecisen ev\n",
            "Epoch 1. Time: 3.747, Train loss: 1.800\n",
            "ct in question any kind of cognizance of| is loftrmong the for this no limpoprion\n",
            "Epoch 2. Time: 3.955, Train loss: 1.658\n",
            "mber to adopt the same moral or custom t|heythmyoge ehisl good spidinided one to \n",
            "Epoch 3. Time: 3.558, Train loss: 1.572\n",
            "ience in reality there exists between re|spure the ordery has he mattrition of th\n",
            "Epoch 4. Time: 3.440, Train loss: 1.511\n",
            "the other old patriot vehemently otherwi|nds others this his be and wascoute thse\n",
            "Epoch 5. Time: 4.087, Train loss: 1.465\n",
            "r hypotheses as fully established i have| is with perce conterves of entail the a\n",
            "Epoch 6. Time: 3.433, Train loss: 1.429\n",
            "e alone who have devised cause sequence |and bearing uped amothing of it is conte\n",
            "Epoch 7. Time: 3.572, Train loss: 1.398\n",
            "or of everything organic inasmuch as all| with the pese tyrancested even all outs\n",
            "Epoch 8. Time: 3.955, Train loss: 1.373\n",
            "ed of explanation was accepted as the tr|oouble hemined stivil in their have disc\n",
            "Epoch 9. Time: 3.785, Train loss: 1.350\n",
            "ecisely our good musicians themselves wr|iek falshisibility at himself ancty now \n",
            "Epoch 10. Time: 3.477, Train loss: 1.332\n",
            "aims and the emancipation of woman insof|ticting live not european can be blent t\n",
            "Epoch 11. Time: 3.618, Train loss: 1.316\n",
            "no right to them in our mouths they are |wemering it was not a read grad which co\n",
            "Epoch 12. Time: 4.174, Train loss: 1.301\n",
            " to his deed he extenuates and maligns i|n the onigatis of the dight the nauls th\n",
            "Epoch 13. Time: 3.611, Train loss: 1.286\n",
            "s have they gradually assumed the appear|ian her any a honoural to the have nothe\n",
            "Epoch 14. Time: 3.604, Train loss: 1.275\n",
            "o find out the subjection to law of natu|rancy reartapies and those own stantapor\n",
            "Epoch 15. Time: 4.187, Train loss: 1.264\n",
            "here there may really be such a thing as| the deceptional question whilo stor to \n",
            "Epoch 16. Time: 3.468, Train loss: 1.253\n",
            " which again creates new perspectives of| the power extrainituents views rancing \n",
            "Epoch 17. Time: 3.554, Train loss: 1.242\n",
            "ch is not to be slain at once and with a|ny divaring there is provers for mutive \n",
            "Epoch 18. Time: 3.871, Train loss: 1.234\n",
            "originality cultur culture civilisation |alse the feeems of his emotion of experi\n",
            "Epoch 19. Time: 3.770, Train loss: 1.225\n",
            "rmously and therewith also the probabili|ty reasonfie indicgered in voltough shor\n",
            "Epoch 20. Time: 3.574, Train loss: 1.217\n",
            "al demands on soul and body that is to s|ay betray seet food an intereding and th\n",
            "Epoch 21. Time: 3.551, Train loss: 1.209\n",
            "ugh of love and goodness in the world to| pass sath mount the religion vanious do\n",
            "Epoch 22. Time: 4.231, Train loss: 1.201\n",
            "ithin one organization as soon however a|s a all it perhaps the passive or short \n",
            "Epoch 23. Time: 3.614, Train loss: 1.195\n",
            "ected by the vicissitudes of generations| will be usely arways how the most unbeh\n",
            "Epoch 24. Time: 3.440, Train loss: 1.187\n",
            "ntercourse with scholars and artists one| badly diver to pomures on the time not \n",
            "Epoch 25. Time: 4.144, Train loss: 1.181\n",
            "ness it is stated in a threatening manne|ring excecting the bodication and true t\n",
            "Epoch 26. Time: 3.731, Train loss: 1.174\n",
            "y i am free he must obey this consciousn|ess as men wrates its even inspire that \n",
            "Epoch 27. Time: 3.633, Train loss: 1.168\n",
            "o their systems in order to make it poss|ible estimas shame prevalels because a m\n",
            "Epoch 28. Time: 3.829, Train loss: 1.163\n",
            "s the so called need of salvation which |stold are really it is he is there truth\n",
            "Epoch 29. Time: 4.032, Train loss: 1.158\n",
            "dared to ask why so apart so alone renou|nd matical pardinged to rectruline contr\n",
            "Epoch 30. Time: 3.496, Train loss: 1.152\n",
            "t is so closely allied to the instinct o|f the elerned against it has been moral \n",
            "Epoch 31. Time: 3.660, Train loss: 1.146\n",
            "ld be thought rational to day nor is the| meroble harden and ranane into existenc\n",
            "Epoch 32. Time: 4.312, Train loss: 1.142\n",
            " which in youth is most typical that tim|es at the ventarious than the elevation \n",
            "Epoch 33. Time: 3.651, Train loss: 1.135\n",
            "ifications are blended novalis one of th|is impeated by spirit of his france up s\n",
            "Epoch 34. Time: 3.634, Train loss: 1.131\n",
            "the many vain and visionary interpretati|on of philosophers ruled in general feel\n",
            "Epoch 35. Time: 4.178, Train loss: 1.126\n",
            "uth pharisaism is not a deterioration of| life the knows it fasts which strict in\n",
            "Epoch 36. Time: 3.439, Train loss: 1.122\n",
            "as still shorter more fleeting and more |religious the memoty that is more found \n",
            "Epoch 37. Time: 3.584, Train loss: 1.118\n",
            "ger familiar with independence of decisi|on a blakent to thought faith the passes\n",
            "Epoch 38. Time: 3.793, Train loss: 1.113\n",
            "simistic confession of faith unless one |green as for the more problem to be ever\n",
            "Epoch 39. Time: 3.864, Train loss: 1.107\n",
            "upon themselves as much blacker and wick|ed the soul to ground even be duty fact \n",
            "Epoch 40. Time: 3.614, Train loss: 1.103\n",
            "f the thing itself in germany the german| science it but as in sip string romatio\n",
            "Epoch 41. Time: 3.607, Train loss: 1.098\n",
            "um and especially hominum can probably a|nd germans long short of the ass our rea\n",
            "Epoch 42. Time: 4.211, Train loss: 1.094\n",
            " than libres penseurs liben pensatori fr|eedom one or an action as thoughts is bo\n",
            "Epoch 43. Time: 3.596, Train loss: 1.090\n",
            "ormed into a longing to master them as i|f wanto christian such an extrafifuelty \n",
            "Epoch 44. Time: 3.463, Train loss: 1.086\n",
            "es of music as still living in order tha|t those timely necessarily not a true th\n",
            "Epoch 45. Time: 4.081, Train loss: 1.081\n",
            "slave insurrection in morals commences i|ndividually therefore of philosophers of\n",
            "Epoch 46. Time: 3.574, Train loss: 1.078\n",
            "than in all the solemn pantomime and tru|mbelone until one and sacrificies i pali\n",
            "Epoch 47. Time: 3.595, Train loss: 1.074\n",
            "m of religious worship is to influence n|ay a sountarinct from the greatness is t\n",
            "Epoch 48. Time: 3.690, Train loss: 1.071\n",
            "o the man in a rage we should be on our |freedom of rank that it is not the virtu\n",
            "Epoch 49. Time: 4.046, Train loss: 1.066\n",
            "m the disease but like every one else be|coming used following the love than this\n",
            "Epoch 50. Time: 3.473, Train loss: 1.063\n",
            "and the day after the morrow has ever fo|llowed that longer men a streenct as amo\n",
            "Epoch 51. Time: 3.594, Train loss: 1.060\n",
            "alizes when in the struggle with danger |he can badly its ourselves are relation \n",
            "Epoch 52. Time: 4.044, Train loss: 1.056\n",
            "ery clearly perceived since it has been |ye may that perhaps mentally in view her\n",
            "Epoch 53. Time: 3.610, Train loss: 1.052\n",
            "otal estimate of him all estimates are s|o crumlated through for how to live that\n",
            "Epoch 54. Time: 3.582, Train loss: 1.050\n",
            "rsely men of modern ideas believe almost| qualizination under the lappoing sacrif\n",
            "Epoch 55. Time: 3.911, Train loss: 1.046\n",
            "t last that it is immoral to say that wh|ich the particuals it is it is an acquac\n",
            "Epoch 56. Time: 3.840, Train loss: 1.043\n",
            "th when he warns all those endowed with |his chaps however nothing it is a backgo\n",
            "Epoch 57. Time: 3.465, Train loss: 1.039\n",
            "ruth absolutely inverted it is so nice a|nd uncredong as repling calls neverthely\n",
            "Epoch 58. Time: 3.581, Train loss: 1.035\n",
            " still professes to belong to the church| general tendeings of obercedemintation \n",
            "Epoch 59. Time: 4.019, Train loss: 1.032\n",
            "ate and go to ruin to acquire qualities |and highness and to should be ultimate e\n",
            "Epoch 60. Time: 3.551, Train loss: 1.030\n",
            "m there is in fine a gradation of rank i|n short sought and brwakest minder best \n",
            "Epoch 61. Time: 3.566, Train loss: 1.026\n",
            "assumed that principal and victim feel a|nd skepticism from indeed the principle \n",
            "Epoch 62. Time: 3.921, Train loss: 1.023\n",
            "ure in man applies to that which has to |the soul of satisfaction which as it has\n",
            "Epoch 63. Time: 3.668, Train loss: 1.021\n",
            " invent hell to send thither those who w|hat seems to not a my worthist of europe\n",
            "Epoch 64. Time: 3.557, Train loss: 1.016\n",
            "losophize to be sure in the case of scho|pence things in the pasts as morality a \n",
            "Epoch 65. Time: 3.490, Train loss: 1.015\n",
            "which to begin with would be distinguish|es the origity as among the spirituality\n",
            "Epoch 66. Time: 4.090, Train loss: 1.011\n",
            "mselves to be incapable of a noble tempo|rary by do this own so feareacy of infin\n",
            "Epoch 67. Time: 3.593, Train loss: 1.008\n",
            "ential thing in heaven and in earth is a| feeling for enegory the present spiritu\n",
            "Epoch 68. Time: 3.604, Train loss: 1.005\n",
            "p rather must a man from whom the ordina|rd of all the most chocised and above th\n",
            "Epoch 69. Time: 4.228, Train loss: 1.003\n",
            " extent of injustice schumann with his t|eath the world so flech bax what fauls h\n",
            "Epoch 70. Time: 3.591, Train loss: 1.002\n",
            "ng type of man as involving his mediocri|talent that i conceal and hater the dege\n",
            "Epoch 71. Time: 3.595, Train loss: 0.997\n",
            " once be more cautious let us be unphilo|somen and penscion and in every synameny\n",
            "Epoch 72. Time: 3.998, Train loss: 0.995\n",
            "ong struggle and wavering to be sure an |exception of a long else carnification o\n",
            "Epoch 73. Time: 3.662, Train loss: 0.993\n",
            "wledge profundity in a woman s mind or j|ustifes awaken of which that themselves \n",
            "Epoch 74. Time: 3.597, Train loss: 0.989\n",
            "have had during a considerable period th|ey whing for it has how strengarly in hi\n",
            "Epoch 75. Time: 3.575, Train loss: 0.988\n",
            "sumes that this is one of the unalterabl|e possibly of the honourable in a teging\n",
            "Epoch 76. Time: 4.157, Train loss: 0.985\n",
            "ry day revert to these primitive process| of real error for then seeks to magnyis\n",
            "Epoch 77. Time: 3.441, Train loss: 0.982\n",
            "f the value of life are illogically deve|loped and powerful after pride is time o\n",
            "Epoch 78. Time: 3.609, Train loss: 0.980\n",
            " he never so near to us can be complete |use its almost at always is the worst th\n",
            "Epoch 79. Time: 4.155, Train loss: 0.977\n",
            "ous training in the disposition the cath|ole s origin of a symptors nobst such as\n",
            "Epoch 80. Time: 3.453, Train loss: 0.975\n",
            "inverted succession of events even from |the goodness before will to however say \n",
            "Epoch 81. Time: 3.586, Train loss: 0.973\n",
            "friends and who the devil also compels y|ou a sympathy for at passifes which maid\n",
            "Epoch 82. Time: 3.831, Train loss: 0.971\n",
            "truth what does woman care for truth fro|m rich for liferable the appronisoming t\n",
            "Epoch 83. Time: 3.879, Train loss: 0.967\n",
            " any mood it almost sets one aglow one l|acksone another the parted a setcise and\n",
            "Epoch 84. Time: 3.591, Train loss: 0.966\n",
            "ad upon such carpets this is provided fo|r aistory of good art deferenced exwsain\n",
            "Epoch 85. Time: 3.617, Train loss: 0.967\n",
            "urage to attain it is tantamount to suff|er further a displays i notwithoutene wh\n",
            "Epoch 86. Time: 4.432, Train loss: 0.962\n",
            "be such and lacks this pure integrity of| every odcular for the basis i almost us\n",
            "Epoch 87. Time: 3.469, Train loss: 0.960\n",
            "atist and historian and besides poet and| impress volurisacces health hypot i do \n",
            "Epoch 88. Time: 3.590, Train loss: 0.958\n",
            " the street and in the heart secrecy sto|ment like us to said in even they had go\n",
            "Epoch 89. Time: 4.103, Train loss: 0.956\n",
            "tagonism to everything privileged and au|thering of need which relfings who infli\n",
            "Epoch 90. Time: 3.669, Train loss: 0.953\n",
            "mong those indifferent persons may be re|finer lessed he wishmen altome as ever m\n",
            "Epoch 91. Time: 3.580, Train loss: 0.952\n",
            "h is calculated to implant a dangerous d|eniamed to like our more thinkers lo exc\n",
            "Epoch 92. Time: 3.668, Train loss: 0.949\n",
            "hanges and changes let us look at the ni|ghten through at most boding time say th\n",
            "Epoch 93. Time: 4.024, Train loss: 0.948\n",
            "e semi barbarians and are only in our hi|gher moment that the conditionces tolian\n",
            "Epoch 94. Time: 3.599, Train loss: 0.946\n",
            " to possess a talent one must also have |beaking of the dictuaries he has truth o\n",
            "Epoch 95. Time: 3.556, Train loss: 0.944\n",
            " the moral epoch of mankind they sacrifi|cation tasked all there is lefings it no\n",
            "Epoch 96. Time: 4.170, Train loss: 0.942\n",
            "ves to certain acts and away from other |possibility of their idea of thought sla\n",
            "Epoch 97. Time: 3.609, Train loss: 0.943\n",
            " beautiful possibilities there may even |in god say lookle effects of contradicat\n",
            "Epoch 98. Time: 3.471, Train loss: 0.938\n",
            "nly and evinces courage directness endur|able to speak a new past adom the value \n",
            "Epoch 99. Time: 3.970, Train loss: 0.938\n",
            "ding ourselves let us try then to relear|es perspervered to require there are pre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Отлично выходит!**"
      ],
      "metadata": {
        "id": "gjz78q6skbqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**!!!LSTM можно тоже использовать в данном коде, простой подстановкой, т.к. результат берется из скрытого состояния**"
      ],
      "metadata": {
        "id": "MTcGlBJllmfn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzUTHB9O_H6J"
      },
      "source": [
        "LSTM действительно училась дольше, но и качество выше, чем у GRU"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}